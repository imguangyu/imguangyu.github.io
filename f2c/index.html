<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="From Frames to Clips: Training-free Adaptive Key Clip Selection for Long-Form Video Understanding">
  <meta name="keywords" content="Video Understanding, Clip Selection, Vision-Language Models, Long-Form Video">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://imguangyu.github.io/" target="_blank">Guangyu Sun</a><sup style="color:#6fbf73;">1,2</sup><span style="font-size: 0.8em;">*</span>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Archit Singhal</a><sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Burak Uzkent</a><sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/" target="_blank">Mubarak Shah</a><sup style="color:#6fbf73;">1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/chenchen/" target="_blank">Chen Chen</a><sup style="color:#ed4b82;">2</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Garin Kessler</a><sup style="color:#6fbf73;">1</sup>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#6fbf73;">1</sup>Amazon</span>
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#ed4b82;">2</sup>University of Central Florida</span><br>
            <span style="font-size: 0.9em; color: #666;">*Work done during an internship at Amazon Prime Video.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.02262"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/imguangyu/F2C"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Figure -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure>
        <img src="../images/f2c.png" alt="F2C Method Overview">
        <figcaption class="has-text-centered">
          <p class="subtitle">
            <strong>Figure 1:</strong> Overview of F2C (From Frames to Clips) method. Instead of selecting isolated key frames, 
            F2C identifies temporally coherent clips that preserve essential motion and event continuity for better video understanding.
          </p>
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video Large Language Models (VLMs) have achieved remarkable results on a variety of vision language tasks, 
            yet their practical use is limited by the "needle in a haystack" problem: the massive number of visual tokens 
            produced from raw video frames exhausts the model's context window. Existing solutions alleviate this issue 
            by selecting a sparse set of frames, thereby reducing token count, but such frame-wise selection discards 
            essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity.
          </p>
          <p>
            In this work we systematically explore the impact of temporal information and demonstrate that extending 
            selection from isolated key frames to key clips, which are short, temporally coherent segments, improves 
            video understanding. To maintain a fixed computational budget while accommodating the larger token footprint 
            of clips, we propose an adaptive resolution strategy that dynamically balances spatial resolution and clip 
            length, ensuring a constant token count per video.
          </p>
          <p>
            Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms 
            uniform sampling up to <strong>8.1%</strong>, <strong>5.6%</strong>, and <strong>10.3%</strong> on Video-MME, 
            LongVideoBench and MLVU benchmarks, respectively. These results highlight the importance of preserving temporal 
            coherence in frame selection and provide a practical pathway for scaling Video LLMs to real world video 
            understanding applications.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Key Contributions -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Clip-based Selection:</strong> We propose extending frame selection to clip selection, preserving 
            temporal coherence and motion information that is crucial for video understanding tasks.</li>
            
            <li><strong>Adaptive Resolution Strategy:</strong> We introduce a novel approach that dynamically balances 
            spatial resolution and clip length to maintain a constant token budget while maximizing temporal information.</li>
            
            <li><strong>Training-free Solution:</strong> Our method requires no additional training and can be applied 
            as a plug-and-play solution to existing Video LLMs, making it practical for real-world deployment.</li>
            
            <li><strong>Comprehensive Evaluation:</strong> We demonstrate significant improvements across three challenging 
            long-form video benchmarks, with gains of up to <strong>10.3%</strong> over uniform sampling baselines.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Key Contributions -->
  </div>
</section>
 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{sun2025frames,
  author    = {Sun, Guangyu and Singhal, Archit and Uzkent, Burak and Shah, Mubarak and Chen, Chen and Kessler, Garin},
  title     = {From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding},
  journal   = {arXiv preprint arXiv:2510.02262},
  year      = {2025},
  note      = {Work done during an internship at Amazon Prime Video}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> template and is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>